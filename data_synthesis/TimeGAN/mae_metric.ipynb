{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ.get(\"tf_upgrade_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tf_upgrade_v2 \\\n",
    "#   --infile /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/timegan.py \\\n",
    "#   --outfile /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/timegan_v2_new.py \\\n",
    "#   --reportfile report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ.get(\"tf_upgrade_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeGAN Tutorial\n",
    "\n",
    "## Time-series Generative Adversarial Networks\n",
    "\n",
    "- Paper: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, \"Time-series Generative Adversarial Networks,\" Neural Information Processing Systems (NeurIPS), 2019.\n",
    "\n",
    "- Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks\n",
    "\n",
    "- Last updated Date: April 24th 2020\n",
    "\n",
    "- Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "\n",
    "This notebook describes the user-guide of a time-series synthetic data generation application using timeGAN framework. We use Stock, Energy, and Sine dataset as examples.\n",
    "\n",
    "### Prerequisite\n",
    "Clone https://github.com/jsyoon0823/timeGAN.git to the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary packages and functions call\n",
    "\n",
    "- timegan: Synthetic time-series data generation module\n",
    "- data_loading: 2 real datasets and 1 synthetic datasets loading and preprocessing\n",
    "- metrics: \n",
    "    - discriminative_metrics: classify real data from synthetic data\n",
    "    - predictive_metrics: train on synthetic, test on real\n",
    "    - visualization: PCA and tSNE analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary packages\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. TimeGAN model\n",
    "from timegan import timegan\n",
    "# 2. Data loading\n",
    "from data_loading import real_data_loading, sine_data_generation\n",
    "# 3. Metrics\n",
    "from metrics.discriminative_metrics import discriminative_score_metrics\n",
    "from metrics.predictive_metrics import predictive_score_metrics\n",
    "from metrics.visualization_metrics import visualization\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intervals(data):\n",
    "    index=data['index']\n",
    "    last_value=index[0]-1\n",
    "    last_index=0\n",
    "    intervals=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        if last_value!=index[i]-1:\n",
    "            intervals.append([last_index,i])\n",
    "            last_value=index[i]\n",
    "            last_index=i\n",
    "        last_value=index[i]\n",
    "    intervals.append([last_index, i])\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(data):\n",
    "    max_len=24\n",
    "    l=len(data)\n",
    "    to_fill=max_len-l\n",
    "    if to_fill!=0:\n",
    "        interval=max_len//to_fill\n",
    "        for j in range(to_fill):\n",
    "            idx=(interval+1)*j+interval\n",
    "            data.insert(min(idx,len(data)-1),float('nan'))\n",
    "    data=pd.Series(data).interpolate(method='polynomial', order=2)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    norm_data = numerator / (denominator + 1e-7)\n",
    "    return np.min(data, 0),np.max(data, 0),norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normlization(data):\n",
    "    normalized_data=(data-data.min())/(data.max()-data.min())\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_of_same_length(data,seq_len):\n",
    "    data_processed=[]\n",
    "    intervals=get_intervals(data)\n",
    "    temp_data=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "#         print(interval)\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:]\n",
    "        for i in range(0, len(data_seg) - seq_len):\n",
    "            _x = data_seg.iloc[i:i + seq_len,:]\n",
    "            temp_data.append(_x)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_prepareation(path):\n",
    "    data=pd.read_csv(path).reset_index()\n",
    "    tics=data['tic'].unique()\n",
    "    # features=[ 'open', 'high', 'low', 'close', 'adjcp','zopen', 'zhigh', 'zlow', 'zadjcp', 'zclose', 'zd_5', 'zd_10',\n",
    "    #    'zd_15', 'zd_20', 'zd_25', 'zd_30', 'pct_return', 'adjcp_filtered',\n",
    "    #    'pct_return_filtered','volume']\n",
    "    features=['open','high','low','close','adjcp','volume']\n",
    "    ret=[]\n",
    "    for col in data.columns:\n",
    "        if col in features:\n",
    "            ret.append(col)\n",
    "    features=ret\n",
    "    min_sclar_by_tic={}\n",
    "    max_sclar_by_tic={}\n",
    "    for tic in tics:\n",
    "        data_by_tic=data.loc[data['tic']==tic,features].astype(float)\n",
    "        min_scalr,max_sclar,norm_data_by_tic=MinMaxScaler(data_by_tic)\n",
    "        # print(min_scalr.shape)\n",
    "        data.loc[data['tic']==tic,features]=norm_data_by_tic\n",
    "        min_sclar_by_tic[tic]=min_scalr\n",
    "        max_sclar_by_tic[tic]=max_sclar\n",
    "    stock_group_num=len(data['stock_type'].unique())\n",
    "    regime_num=len(data['label'].unique())\n",
    "    for tic in tics:\n",
    "        with open('./data/scalr/'+str(tic)+'_minsclar'+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(min_sclar_by_tic[tic], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('./data/scalr/'+str(tic)+'_maxsclar'+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(max_sclar_by_tic[tic], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        for j in range(regime_num):\n",
    "            data_seg=data.loc[(data['tic']==tic) & (data['label']==j),['index','open','high','low','close','adjcp','volume']]\n",
    "    #         data_dict[(i,j)]=data_seg\n",
    "            data_seg.to_csv('./data/data_seg_'+tic+'_'+str(j)+'.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load original dataset and preprocess the loaded data.\n",
    "\n",
    "- data_name: stock, energy, or sine\n",
    "- seq_len: sequence length of the time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_acl18_ori_path='/data/home/zwt/mae_st_rebuild/data/acl18_inter/features/'\n",
    "mae_acl18_generated_path='/home/hcxia/mae_st_reframe_pretrained_eval/mae_st_reframe/pretrained_simmim/output_dir/exp05_patchsize1x1/simmim_pretrain/vit_maskratio05/generated_data/epoch_1600/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pathlist = Path(mae_acl18_ori_path).rglob(\"*\")\n",
    "mae_acl18_inter_ori_dict={}\n",
    "mae_acl18_inter_ori_dict['all']=[]\n",
    "mae_acl18_inter_ori_small_dict={}\n",
    "mae_acl18_inter_ori_small_dict['all']=[]\n",
    "features = [\n",
    "    'open',\n",
    "    'high',\n",
    "    'low',\n",
    "    'close',\n",
    "    'kmid2',\n",
    "    'kup2',\n",
    "    'klow',\n",
    "    'klow2',\n",
    "    'ksft2',\n",
    "    'roc_5',\n",
    "    'roc_10',\n",
    "    'roc_20',\n",
    "    'roc_30',\n",
    "    'roc_60',\n",
    "    'ma_5',\n",
    "    'ma_10',\n",
    "    'ma_20',\n",
    "    'ma_30',\n",
    "    'ma_60',\n",
    "    'std_5',\n",
    "    'std_10',\n",
    "    'std_20',\n",
    "    'std_30',\n",
    "    'std_60',\n",
    "    'beta_5',\n",
    "    'beta_10',\n",
    "    'beta_20',\n",
    "    'beta_30',\n",
    "    'beta_60',\n",
    "    'max_5',\n",
    "    'max_10',\n",
    "    'max_20',\n",
    "    'max_30',\n",
    "    'max_60',\n",
    "    'min_5',\n",
    "    'min_10',\n",
    "    'min_20',\n",
    "    'min_30',\n",
    "    'min_60',\n",
    "    'qtlu_5',\n",
    "    'qtlu_10',\n",
    "    'qtlu_20',\n",
    "    'qtlu_30',\n",
    "    'qtlu_60',\n",
    "    'qtld_5',\n",
    "    'qtld_10',\n",
    "    'qtld_20',\n",
    "    'qtld_30',\n",
    "    'qtld_60',\n",
    "    'rank_5',\n",
    "    'rank_10',\n",
    "    'rank_20',\n",
    "    'rank_30',\n",
    "    'rank_60',\n",
    "    'imax_5',\n",
    "    'imax_10',\n",
    "    'imax_20',\n",
    "    'imax_30',\n",
    "    'imax_60',\n",
    "    'imin_5',\n",
    "    'imin_10',\n",
    "    'imin_20',\n",
    "    'imin_30',\n",
    "    'imin_60',\n",
    "    'imxd_5',\n",
    "    'imxd_10',\n",
    "    'imxd_20',\n",
    "    'imxd_30',\n",
    "    'imxd_60',\n",
    "    'cntp_5',\n",
    "    'cntp_10',\n",
    "    'cntp_20',\n",
    "    'cntp_30',\n",
    "    'cntp_60',\n",
    "    'cntn_5',\n",
    "    'cntn_10',\n",
    "    'cntn_20',\n",
    "    'cntn_30',\n",
    "    'cntn_60',\n",
    "    'cntd_5',\n",
    "    'cntd_10',\n",
    "    'cntd_20',\n",
    "    'cntd_30',\n",
    "    'cntd_60',\n",
    "    'sump_5',\n",
    "    'sump_10',\n",
    "    'sump_20',\n",
    "    'sump_30',\n",
    "    'sump_60',\n",
    "    'sumn_5',\n",
    "    'sumn_10',\n",
    "    'sumn_20',\n",
    "    'sumn_30',\n",
    "    'sumn_60',\n",
    "    'sumd_5',\n",
    "    'sumd_10',\n",
    "    'sumd_20',\n",
    "    'sumd_30',\n",
    "    'sumd_60',\n",
    "]\n",
    "seq_len=24\n",
    "for path in pathlist:\n",
    "    data=pd.read_csv(path).loc[:,features]\n",
    "    for i in range(0, len(data) - seq_len):\n",
    "            _x = data.iloc[i:i + seq_len,:].to_numpy()\n",
    "            _x_small = data.iloc[i:i + seq_len,:4].to_numpy()\n",
    "            mae_acl18_inter_ori_dict['all'].append(_x)\n",
    "            mae_acl18_inter_ori_small_dict['all'].append(_x_small)\n",
    "    # print(mae_acl18_inter_ori_small_dict['all'][-1].shape)\n",
    "\n",
    "display(len(mae_acl18_inter_ori_dict['all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18054, 24, 99)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(18054, 24, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pathlist = Path(mae_acl18_generated_path).rglob(\"*\")\n",
    "mae_acl18_inter_generated_dict={}\n",
    "mae_acl18_inter_generated_dict['all']=[]\n",
    "mae_acl18_inter_generated_small_dict={}\n",
    "mae_acl18_inter_generated_small_dict['all']=[]\n",
    "for path in pathlist:\n",
    "    data=np.load(path)\n",
    "    mae_acl18_inter_generated_dict['all'].append(data)\n",
    "    mae_acl18_inter_generated_small_dict['all'].append(data[:,:4])\n",
    "    # print(mae_acl18_inter_generated_dict['all'][-1].shape)\n",
    "mae_acl18_inter_generated_dict['all']=np.stack(mae_acl18_inter_generated_dict['all'],axis=0)\n",
    "mae_acl18_inter_generated_small_dict['all']=np.stack(mae_acl18_inter_generated_small_dict['all'],axis=0)\n",
    "display(mae_acl18_inter_generated_dict['all'].shape)\n",
    "display(mae_acl18_inter_generated_small_dict['all'].shape)\n",
    "    # print(data.shape)\n",
    "    # print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the generated data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_score(ori_data, generated_data):\n",
    "    metric_iteration = 5\n",
    "    discriminative_score = list()\n",
    "    for _ in range(metric_iteration):\n",
    "        temp_disc = discriminative_score_metrics(ori_data, generated_data)\n",
    "        discriminative_score.append(temp_disc)\n",
    "    print('Discriminative score: ' + str(np.round(np.mean(discriminative_score), 4)))\n",
    "    return np.round(np.mean(discriminative_score), 4)\n",
    "\n",
    "def pred_score(ori_data, generated_data):\n",
    "    predictive_score = list()\n",
    "    metric_iteration = 5\n",
    "    for tt in range(metric_iteration):\n",
    "        temp_pred = predictive_score_metrics(ori_data, generated_data)\n",
    "        predictive_score.append(temp_pred)   \n",
    "    print('Predictive score: ' + str(np.round(np.mean(predictive_score), 4)))\n",
    "    return np.round(np.mean(predictive_score), 4)\n",
    "    \n",
    "def evaluation(data_set_dict,genrated_data_set_dict):\n",
    "    length_dict={}\n",
    "    discriminative_score_dict={}\n",
    "    predictive_score_dict={}\n",
    "    for dataset_name, data_set in data_set_dict.items():\n",
    "        length_dict[dataset_name]=len(data_set_dict[dataset_name])\n",
    "        # print(len(data_set_dict[dataset_name]),len(genrated_data_set_dict[dataset_name]))\n",
    "        discriminative_score_dict[dataset_name] = dis_score(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name])\n",
    "        predictive_score_dict[dataset_name] = pred_score(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name])\n",
    "        visualization(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name], 'pca')\n",
    "        visualization(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name], 'tsne')\n",
    "    return length_dict,discriminative_score_dict,predictive_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 99)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_acl18_inter_ori_dict['all'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18054, 24, 99)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_acl18_inter_generated_dict['all'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:39: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:57: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:76: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:76: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:77: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:78: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/hcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/hcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/hcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/hcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:81: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:96: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:100: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 16:17:16.870935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-23 16:17:16.871099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-23 16:17:16.871305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: NVIDIA RTX A6000 major: 8 minor: 6 memoryClockRate(GHz): 1.8\n",
      "pciBusID: 0000:21:00.0\n",
      "2023-01-23 16:17:16.871372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-23 16:17:16.871521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \n",
      "name: NVIDIA RTX A6000 major: 8 minor: 6 memoryClockRate(GHz): 1.8\n",
      "pciBusID: 0000:22:00.0\n",
      "2023-01-23 16:17:16.871566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-23 16:17:16.871711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: \n",
      "name: NVIDIA RTX A6000 major: 8 minor: 6 memoryClockRate(GHz): 1.8\n",
      "pciBusID: 0000:41:00.0\n",
      "2023-01-23 16:17:16.871747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-23 16:17:16.871890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: \n",
      "name: NVIDIA RTX A6000 major: 8 minor: 6 memoryClockRate(GHz): 1.8\n",
      "pciBusID: 0000:43:00.0\n",
      "2023-01-23 16:17:16.872003: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-23 16:17:16.872049: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-23 16:17:16.872081: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-23 16:17:16.872113: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-23 16:17:16.872143: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-23 16:17:16.872171: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-23 16:17:16.872199: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-23 16:17:16.872202: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-01-23 16:17:16.872367: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2023-01-23 16:17:16.899124: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2694945000 Hz\n",
      "2023-01-23 16:17:16.903207: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xf08b210 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-23 16:17:16.903235: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:101: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 16:17:17.094294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-23 16:17:17.181483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-23 16:17:17.182264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-23 16:17:17.182743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-23 16:17:17.183092: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9090c50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-23 16:17:17.183103: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-01-23 16:17:17.183106: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-01-23 16:17:17.183108: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-01-23 16:17:17.183111: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-01-23 16:17:17.183369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:17:17.183378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:17:29.594585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:17:29.594612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:17:42.138725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:17:42.138754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:17:54.421901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:17:54.421929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:18:07.070997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:18:07.071028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminative score: 0.5\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/predictive_metrics.py:81: The name tf.losses.absolute_difference is deprecated. Please use tf.compat.v1.losses.absolute_difference instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 16:18:18.934224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:18:18.934246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:18:51.480311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:18:51.480338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:19:26.764385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:19:26.764502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:19:59.343473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:19:59.343499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:20:32.429122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:20:32.429148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive score: 84.4344\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 47010 is out of bounds for axis 0 with size 18054",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_188545/1890940326.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmae_acl18_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae_acl18_inter_ori_small_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmae_acl18_inter_generated_small_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_188545/1672263469.py\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(data_set_dict, genrated_data_set_dict)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdiscriminative_score_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpredictive_score_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mvisualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pca'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mvisualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tsne'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlength_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscriminative_score_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictive_score_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/visualization_metrics.py\u001b[0m in \u001b[0;36mvisualization\u001b[0;34m(ori_data, generated_data, analysis, savefig)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mori_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mori_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0mgenerated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mori_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 47010 is out of bounds for axis 0 with size 18054"
     ]
    }
   ],
   "source": [
    "mae_acl18_res=evaluation(mae_acl18_inter_ori_small_dict,mae_acl18_inter_generated_small_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 16:30:28.602227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:30:28.602255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:31:04.012652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:31:04.012682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:31:39.136727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:31:39.136755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:32:14.456377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:32:14.456408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-23 16:32:50.432951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:32:50.432975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminative score: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 16:33:25.809757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:33:25.809855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
     ]
    }
   ],
   "source": [
    "mae_acl18_res=evaluation(mae_acl18_inter_ori_dict,mae_acl18_inter_generated_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mae_acl18_res,mae_acl18_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(GOOG_data_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(AAPL_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(group_3_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_data_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(group_4_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_res={}\n",
    "for tic in data_dict_tic_res.keys():\n",
    "    display(data_dict_tic_res[tic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(GOOG_processed_data,GOOG_genrated_data, 'pca')\n",
    "visualization(GOOG_processed_data,GOOG_genrated_data, 'tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in data_dict.keys():\n",
    "    print('vis of '+k)\n",
    "    visualization(data_dict[k], generated_data_dict[k], 'pca')\n",
    "    visualization(data_dict[k], generated_data_dict[k], 'tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leverage Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].pct_return.to_numpy()\n",
    "        std=data_seg.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tic in tics:\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+tic+'_'+str(j)+'.csv')\n",
    "        print('stock',tic,'regime',j)\n",
    "        std_list=get_std_list(data)\n",
    "        display(pd.DataFrame(std_list).describe()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by stock group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv')\n",
    "        print('stock',i,'regime',j)\n",
    "        std_list=get_std_list(data)\n",
    "        display(pd.DataFrame(std_list).describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].to_numpy()\n",
    "        std=data_seg.adj.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# key takeaway\n",
    "1. regime 0 has high variance mean and high variance variance\n",
    "2. regime 1 has low variance mean and low variance variance\n",
    "3. regime 2 has low variance mean and low/high variance variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train Static learning classification discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sktime.classification.kernel_based import RocketClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_of_same_length_df(data,seq_len):\n",
    "    data_processed=[]\n",
    "    intervals=get_intervals(data)\n",
    "    temp_data=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:]\n",
    "        for i in range(0, len(data_seg) - seq_len):\n",
    "            _x = data[i:i + seq_len]\n",
    "            temp_data.append(_x)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data_seg_'+\"0\"+'_'+\"0\"+'.csv')\n",
    "display(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tic in tics:\n",
    "    print(tic)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+tic+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length_df(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend(process_data)\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    classifier = RocketClassifier(num_kernels=2000,n_jobs=-1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL single stock classification have unbelieve 100% acc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    print('stock_group',i)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length_df(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend(process_data)\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    classifier = RocketClassifier(num_kernels=2000,n_jobs=-1,use_multivariate='yes')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still 1.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# classifier = TimeSeriesForestClassifier()\n",
    "# classifier.fit(X_train, y_train)\n",
    "# y_pred = classifier.predict(X_test)\n",
    "# accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train Deep learning classification discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tsai.all import *\n",
    "my_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    print('stock_group',i)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend([p.transpose() for p in process_data])\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X=np.array(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    X, y, splits = combine_split_data([X_train, X_test], [y_train, y_test])\n",
    "    tfms  = [None, [Categorize()]]\n",
    "    dsets = TSDatasets(X, y, tfms=tfms, splits=splits, inplace=True)\n",
    "    dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=[TSStandardize()], num_workers=0)\n",
    "    model = InceptionTime(dls.vars, dls.c)\n",
    "    learn = Learner(dls, model, metrics=accuracy)\n",
    "    learn.fit_one_cycle(25, lr_max=1e-3)\n",
    "    learn.plot_metrics()\n",
    "    learn.save_all(path='export', dls_fname='dls_'+str(i)+'_'+str(j), model_fname='model_'+str(i)+'_'+str(j), learner_fname='learner_'+str(i)+'_'+str(j))\n",
    "#     display(type(X_train),X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key takeaway\n",
    "\n",
    "InceptionTime can do the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, splits = get_classification_data('LSST', split_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms  = [None, TSClassification()] # TSClassification == Categorize\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, new_y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=[64, 128])\n",
    "dls.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].to_numpy()\n",
    "        std=data_seg.adj.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = TSStandardize(by_sample=True)\n",
    "mv_clf = TSClassifier(X, y, splits=splits, path='models', arch=InceptionTimePlus, batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph())\n",
    "mv_clf.fit_one_cycle(10, 1e-2)\n",
    "mv_clf.export(\"mv_clf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

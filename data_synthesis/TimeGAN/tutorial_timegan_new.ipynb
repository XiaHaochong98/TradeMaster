{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ.get(\"tf_upgrade_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tf_upgrade_v2 \\\n",
    "#   --infile /home/zcxia/TradeMaster/data_synthesis/TimeGAN/timegan.py \\\n",
    "#   --outfile /home/zcxia/TradeMaster/data_synthesis/TimeGAN/timegan_v2.py \\\n",
    "#   --reportfile report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ.get(\"tf_upgrade_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeGAN Tutorial\n",
    "\n",
    "## Time-series Generative Adversarial Networks\n",
    "\n",
    "- Paper: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, \"Time-series Generative Adversarial Networks,\" Neural Information Processing Systems (NeurIPS), 2019.\n",
    "\n",
    "- Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks\n",
    "\n",
    "- Last updated Date: April 24th 2020\n",
    "\n",
    "- Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "\n",
    "This notebook describes the user-guide of a time-series synthetic data generation application using timeGAN framework. We use Stock, Energy, and Sine dataset as examples.\n",
    "\n",
    "### Prerequisite\n",
    "Clone https://github.com/jsyoon0823/timeGAN.git to the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary packages and functions call\n",
    "\n",
    "- timegan: Synthetic time-series data generation module\n",
    "- data_loading: 2 real datasets and 1 synthetic datasets loading and preprocessing\n",
    "- metrics: \n",
    "    - discriminative_metrics: classify real data from synthetic data\n",
    "    - predictive_metrics: train on synthetic, test on real\n",
    "    - visualization: PCA and tSNE analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary packages\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. TimeGAN model\n",
    "from timegan import timegan\n",
    "# 2. Data loading\n",
    "from data_loading import real_data_loading, sine_data_generation\n",
    "# 3. Metrics\n",
    "from metrics.discriminative_metrics import discriminative_score_metrics\n",
    "from metrics.predictive_metrics import predictive_score_metrics\n",
    "from metrics.visualization_metrics import visualization\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import importlib\n",
    "# importlib.reload(styletimegan)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intervals(data):\n",
    "    index=data['index']\n",
    "    last_value=index[0]-1\n",
    "    last_index=0\n",
    "    intervals=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        if last_value!=index[i]-1:\n",
    "            intervals.append([last_index,i])\n",
    "            last_value=index[i]\n",
    "            last_index=i\n",
    "        last_value=index[i]\n",
    "    intervals.append([last_index, i])\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(data):\n",
    "    max_len=24\n",
    "    l=len(data)\n",
    "    to_fill=max_len-l\n",
    "    if to_fill!=0:\n",
    "        interval=max_len//to_fill\n",
    "        for j in range(to_fill):\n",
    "            idx=(interval+1)*j+interval\n",
    "            data.insert(min(idx,len(data)-1),float('nan'))\n",
    "    data=pd.Series(data).interpolate(method='polynomial', order=2)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    norm_data = numerator / (denominator + 1e-7)\n",
    "    return np.min(data, 0),np.max(data, 0),norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normlization(data):\n",
    "    normalized_data=(data-data.min())/(data.max()-data.min())\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_of_same_length(data,seq_len):\n",
    "    data_processed=[]\n",
    "    intervals=get_intervals(data)\n",
    "    temp_data=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "#         print(interval)\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:]\n",
    "        for i in range(0, len(data_seg) - seq_len):\n",
    "            _x = data_seg.iloc[i:i + seq_len,:]\n",
    "            temp_data.append(_x)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_prepareation(path):\n",
    "    data=pd.read_csv(path).reset_index()\n",
    "    tics=data['tic'].unique()\n",
    "    # features=[ 'open', 'high', 'low', 'close', 'adjcp','zopen', 'zhigh', 'zlow', 'zadjcp', 'zclose', 'zd_5', 'zd_10',\n",
    "    #    'zd_15', 'zd_20', 'zd_25', 'zd_30', 'pct_return', 'adjcp_filtered',\n",
    "    #    'pct_return_filtered','volume']\n",
    "    features=['open','high','low','close','adjcp','volume']\n",
    "    ret=[]\n",
    "    for col in data.columns:\n",
    "        if col in features:\n",
    "            ret.append(col)\n",
    "    features=ret\n",
    "    min_sclar_by_tic={}\n",
    "    max_sclar_by_tic={}\n",
    "    for tic in tics:\n",
    "        data_by_tic=data.loc[data['tic']==tic,features].astype(float)\n",
    "        min_scalr,max_sclar,norm_data_by_tic=MinMaxScaler(data_by_tic)\n",
    "        # print(min_scalr.shape)\n",
    "        data.loc[data['tic']==tic,features]=norm_data_by_tic\n",
    "        min_sclar_by_tic[tic]=min_scalr\n",
    "        max_sclar_by_tic[tic]=max_sclar\n",
    "    stock_group_num=len(data['stock_type'].unique())\n",
    "    regime_num=len(data['label'].unique())\n",
    "    for tic in tics:\n",
    "        with open('./data/scalr/'+str(tic)+'_minsclar'+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(min_sclar_by_tic[tic], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('./data/scalr/'+str(tic)+'_maxsclar'+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(max_sclar_by_tic[tic], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        for j in range(regime_num):\n",
    "            data_seg=data.loc[(data['tic']==tic) & (data['label']==j),['index','open','high','low','close','adjcp','volume']]\n",
    "    #         data_dict[(i,j)]=data_seg\n",
    "            data_seg.to_csv('./data/data_seg_'+tic+'_'+str(j)+'.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load original dataset and preprocess the loaded data.\n",
    "\n",
    "- data_name: stock, energy, or sine\n",
    "- seq_len: sequence length of the time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv('/home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/GOOG_labeled_3_24.csv').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv(\"./DJI_all_labeled_3_24.csv\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_prepareation(\"/home/hcxia/TradeMaster_dev/TradeMaster/data/data/other/DJI_all_labeled_3_24.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_prepareation('/home/hcxia/TradeMaster_dev/TradeMaster/data/data/other/GOOG_labeled_3_24.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data by stock group and regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize features with in own tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeGan Dataset description:\n",
    "Stocks. By contrast, sequences of stock prices are continuous-valued but aperiodic; furthermore,\n",
    "features are correlated with each other. We use the daily historical Google stocks data from 2004 to\n",
    "2019, including as features the volume and high, low, opening, closing, and adjusted closing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_trainning(path):\n",
    "    data=pd.read_csv(path).drop('index', axis=1)\n",
    "    data=data.reset_index().rename(columns={data.index.name:'index'})\n",
    "#     plt.plot(data['adjcp'])\n",
    "#     plt.show()\n",
    "#     display(data.head())\n",
    "    data.to_csv('temp_data.csv')\n",
    "    data=get_data_of_same_length(data,24)\n",
    "#     for i in range(20):\n",
    "#         plt.plot(data[i]['adjcp'])\n",
    "#         plt.show()\n",
    "#         print(i)\n",
    "#         display(data[i])\n",
    "#     display(len(data))\n",
    "    data=[d.loc[:,['open','high','low','close','adjcp','volume']].to_numpy() for d in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOG_data={}\n",
    "for i in range(3):\n",
    "    GOOG_data['data_seg_GOOG_'+str(i)]=prepare_data_for_trainning('./data/data_seg_GOOG_'+str(i)+'.csv')\n",
    "    print(i,len(GOOG_data['data_seg_GOOG_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../../data/data/other/DJI_all_labeled_3_24.csv\").reset_index()\n",
    "tics=data['tic'].unique()\n",
    "data_dict_tic={}\n",
    "for tic in tics:\n",
    "    data_dict_tic[tic]={}\n",
    "    for i in range(3):\n",
    "        data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)]=prepare_data_for_trainning('./data/data_seg_'+str(tic)+'_'+str(i)+'.csv')\n",
    "        print(tic,i,len(data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../../data/data/other/DJI_all_labeled_3_24.csv\").reset_index()\n",
    "# display(data.columns)\n",
    "tic_group_pair=data.loc[:,['tic','stock_type']]\n",
    "tic_group_pair=tic_group_pair.groupby(['tic','stock_type']).size().reset_index(name='Freq')\n",
    "stock_group_num=len(data['stock_type'].unique())\n",
    "tic_in_group={}\n",
    "for group in range(stock_group_num):\n",
    "#     if group not in tic_in_group：\n",
    "#         tic_in_group[group]=[]\n",
    "    tic_in_group[group]=list(tic_group_pair.loc[tic_group_pair['stock_type']==group,:]['tic'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tic_in_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../../data/data/other/DJI_all_labeled_3_24.csv\").reset_index()\n",
    "stock_group_num=len(data['stock_type'].unique())\n",
    "data_dict_group={}\n",
    "for group in range(stock_group_num):\n",
    "    data_dict_group[group]={}\n",
    "    for i in range(3):\n",
    "        if 'data_seg_'+str(group)+'_'+str(i) not in data_dict_group[group]:\n",
    "            data_dict_group[group]['data_seg_'+str(group)+'_'+str(i)]=[]\n",
    "        for tic in tic_in_group[group]:\n",
    "            data_dict_group[group]['data_seg_'+str(group)+'_'+str(i)].extend(data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)])\n",
    "        print(group,i,len(data_dict_group[group]['data_seg_'+str(group)+'_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All dji stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all={}\n",
    "for i in range(3):\n",
    "    if 'data_seg_'+'all'+'_'+str(i) not in data_all:\n",
    "        data_all['data_seg_'+'all'+'_'+str(i)]=[]\n",
    "    for tic in tics:\n",
    "        data_all['data_seg_'+'all'+'_'+str(i)].extend(data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)])\n",
    "    print(i,len(data_all['data_seg_'+'all'+'_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data set:\n",
    "\n",
    "- GOOG_data\n",
    "- data_dict_tic (dict of dict by tic)\n",
    "- data_dict_group (dict of dict by group num)\n",
    "- data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set network parameters\n",
    "\n",
    "TimeGAN network parameters should be optimized for different datasets.\n",
    "\n",
    "- module: gru, lstm, or lstmLN\n",
    "- hidden_dim: hidden dimensions\n",
    "- num_layer: number of layers\n",
    "- iteration: number of training iterations\n",
    "- batch_size: the number of samples in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GOOG_data.keys())\n",
    "print(data_dict_tic.keys())\n",
    "print(data_dict_group.keys())\n",
    "print(data_all.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(data_set_dict):\n",
    "    parameters = dict()\n",
    "    parameters['module'] = 'gru' \n",
    "    parameters['hidden_dim'] = 24\n",
    "    parameters['num_layer'] = 3\n",
    "    parameters['iterations'] = 10000\n",
    "    parameters['batch_size'] = 128\n",
    "    gnerated_dataset_dict={}\n",
    "    for dataset_name, data_set in data_set_dict.items():\n",
    "        print(str(dataset_name))\n",
    "        gnerated_dataset_dict[dataset_name] = timegan(data_set, parameters,device=1,save_name=str(dataset_name))   \n",
    "    return gnerated_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_generated={}\n",
    "for tic in data_dict_tic.keys():\n",
    "    if tic not in data_dict_tic_generated:\n",
    "        data_dict_tic_generated[tic]=training(data_dict_tic[tic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_seg_GOOG_0\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:57: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:100: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:114: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:114: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/utils.py:95: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:115: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:116: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /data/home/zcxia/data/home/zcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /data/home/zcxia/data/home/zcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /data/home/zcxia/data/home/zcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /data/home/zcxia/data/home/zcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /data/home/zcxia/data/home/zcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:225: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:232: The name tf.losses.sigmoid_cross_entropy is deprecated. Please use tf.compat.v1.losses.sigmoid_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:243: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:261: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:268: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:269: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 20:55:02.859944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-25 20:55:02.919303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: GeForce RTX 3090 major: 8 minor: 6 memoryClockRate(GHz): 1.695\n",
      "pciBusID: 0000:3b:00.0\n",
      "2023-01-25 20:55:02.919443: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-25 20:55:02.919490: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-25 20:55:02.919534: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-25 20:55:02.919580: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-25 20:55:02.919620: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-25 20:55:02.919661: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-25 20:55:02.919701: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-25 20:55:02.919706: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-01-25 20:55:02.920793: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2023-01-25 20:55:02.934963: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz\n",
      "2023-01-25 20:55:02.940188: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5f01f80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-25 20:55:02.940239: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-01-25 20:55:03.111409: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fefba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-25 20:55:03.111456: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-01-25 20:55:03.111583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-25 20:55:03.111593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:272: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/zcxia/TM/TradeMaster/data_synthesis/TimeGAN/timegan.py:306: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /data/home/zcxia/data/home/zcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "GOOG_data_generated=training(GOOG_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_APPL_generated=training(data_dict_tic['AAPL'])\n",
    "data_dict_group_3_generated=training(data_dict_group[3])\n",
    "GOOG_data_generated=training(GOOG_data)\n",
    "data_all_generated=training(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_group_4_generated=training(data_dict_group[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save=[\n",
    "data_dict_tic_APPL_generated,\n",
    "data_dict_group_3_generated,\n",
    "GOOG_data_generated,\n",
    "data_all_generated,\n",
    "data_dict_group_4_generated\n",
    "]\n",
    "for tic in data_dict_tic_generated.keys():\n",
    "    locals()['data_dict_tic_'+tic+'_generated']=data_dict_tic_generated[tic]\n",
    "    data_to_save.append(locals()['data_dict_tic_'+tic+'_generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inspect\n",
    "\n",
    "    \n",
    "def retrieve_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "for i in range(len(data_to_save)):\n",
    "    try:\n",
    "        with open('./generated_data/'+retrieve_name(data_to_save[i])[-1]+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(data_to_save[i], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('writen to '+retrieve_name(data_to_save[i])[-1]+'.pickle')\n",
    "    except:\n",
    "        print('skip this')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the generated data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_score(ori_data, generated_data):\n",
    "    metric_iteration = 5\n",
    "    discriminative_score = list()\n",
    "    for _ in range(metric_iteration):\n",
    "        temp_disc = discriminative_score_metrics(ori_data, generated_data)\n",
    "        discriminative_score.append(temp_disc)\n",
    "    print('Discriminative score: ' + str(np.round(np.mean(discriminative_score), 4)))\n",
    "    return np.round(np.mean(discriminative_score), 4)\n",
    "\n",
    "def pred_score(ori_data, generated_data):\n",
    "    predictive_score = list()\n",
    "    metric_iteration = 5\n",
    "    for tt in range(metric_iteration):\n",
    "        temp_pred = predictive_score_metrics(ori_data, generated_data)\n",
    "        predictive_score.append(temp_pred)   \n",
    "    print('Predictive score: ' + str(np.round(np.mean(predictive_score), 4)))\n",
    "    return np.round(np.mean(predictive_score), 4)\n",
    "    \n",
    "def evaluation(data_set_dict,genrated_data_set_dict):\n",
    "    length_dict={}\n",
    "    discriminative_score_dict={}\n",
    "    predictive_score_dict={}\n",
    "    for dataset_name, data_set in data_set_dict.items():\n",
    "        length_dict[dataset_name]=len(data_set_dict[dataset_name])\n",
    "        discriminative_score_dict[dataset_name] = dis_score(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name])\n",
    "        predictive_score_dict[dataset_name] = pred_score(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name])\n",
    "        visualization(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name], 'pca')\n",
    "        visualization(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name], 'tsne')\n",
    "    return length_dict,discriminative_score_dict,predictive_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_res={}\n",
    "for tic in data_dict_tic.keys():\n",
    "    data_dict_tic_res[tic]=[evaluation(data_dict_tic[tic],data_dict_tic_generated[tic])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_APPL_generated=training(data_dict_tic['AAPL'])\n",
    "data_dict_group_3_generated=training(data_dict_group[3])\n",
    "GOOG_data_generated=training(GOOG_data)\n",
    "data_all_generated=training(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_res=[evaluation(data_dict_tic['AAPL'],data_dict_tic_APPL_generated)]\n",
    "group_3_res=[evaluation(data_dict_group[3],data_dict_group_3_generated)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOG_data_res=[evaluation(GOOG_data,GOOG_data_generated)]\n",
    "all_data_res=[evaluation(data_all,data_all_generated)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_4_res=[evaluation(data_dict_group[4],data_dict_group_4_generated)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(GOOG_data_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(AAPL_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(group_3_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_data_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(group_4_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_res={}\n",
    "for tic in data_dict_tic_res.keys():\n",
    "    display(data_dict_tic_res[tic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(GOOG_processed_data,GOOG_genrated_data, 'pca')\n",
    "visualization(GOOG_processed_data,GOOG_genrated_data, 'tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in data_dict.keys():\n",
    "    print('vis of '+k)\n",
    "    visualization(data_dict[k], generated_data_dict[k], 'pca')\n",
    "    visualization(data_dict[k], generated_data_dict[k], 'tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leverage Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].pct_return.to_numpy()\n",
    "        std=data_seg.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tic in tics:\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+tic+'_'+str(j)+'.csv')\n",
    "        print('stock',tic,'regime',j)\n",
    "        std_list=get_std_list(data)\n",
    "        display(pd.DataFrame(std_list).describe()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by stock group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv')\n",
    "        print('stock',i,'regime',j)\n",
    "        std_list=get_std_list(data)\n",
    "        display(pd.DataFrame(std_list).describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].to_numpy()\n",
    "        std=data_seg.adj.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# key takeaway\n",
    "1. regime 0 has high variance mean and high variance variance\n",
    "2. regime 1 has low variance mean and low variance variance\n",
    "3. regime 2 has low variance mean and low/high variance variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train Static learning classification discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sktime.classification.kernel_based import RocketClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_of_same_length_df(data,seq_len):\n",
    "    data_processed=[]\n",
    "    intervals=get_intervals(data)\n",
    "    temp_data=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:]\n",
    "        for i in range(0, len(data_seg) - seq_len):\n",
    "            _x = data[i:i + seq_len]\n",
    "            temp_data.append(_x)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data_seg_'+\"0\"+'_'+\"0\"+'.csv')\n",
    "display(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tic in tics:\n",
    "    print(tic)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+tic+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length_df(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend(process_data)\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    classifier = RocketClassifier(num_kernels=2000,n_jobs=-1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL single stock classification have unbelieve 100% acc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    print('stock_group',i)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length_df(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend(process_data)\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    classifier = RocketClassifier(num_kernels=2000,n_jobs=-1,use_multivariate='yes')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still 1.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# classifier = TimeSeriesForestClassifier()\n",
    "# classifier.fit(X_train, y_train)\n",
    "# y_pred = classifier.predict(X_test)\n",
    "# accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train Deep learning classification discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tsai.all import *\n",
    "my_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    print('stock_group',i)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend([p.transpose() for p in process_data])\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X=np.array(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    X, y, splits = combine_split_data([X_train, X_test], [y_train, y_test])\n",
    "    tfms  = [None, [Categorize()]]\n",
    "    dsets = TSDatasets(X, y, tfms=tfms, splits=splits, inplace=True)\n",
    "    dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=[TSStandardize()], num_workers=0)\n",
    "    model = InceptionTime(dls.vars, dls.c)\n",
    "    learn = Learner(dls, model, metrics=accuracy)\n",
    "    learn.fit_one_cycle(25, lr_max=1e-3)\n",
    "    learn.plot_metrics()\n",
    "    learn.save_all(path='export', dls_fname='dls_'+str(i)+'_'+str(j), model_fname='model_'+str(i)+'_'+str(j), learner_fname='learner_'+str(i)+'_'+str(j))\n",
    "#     display(type(X_train),X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key takeaway\n",
    "\n",
    "InceptionTime can do the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, splits = get_classification_data('LSST', split_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms  = [None, TSClassification()] # TSClassification == Categorize\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, new_y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=[64, 128])\n",
    "dls.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].to_numpy()\n",
    "        std=data_seg.adj.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = TSStandardize(by_sample=True)\n",
    "mv_clf = TSClassifier(X, y, splits=splits, path='models', arch=InceptionTimePlus, batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph())\n",
    "mv_clf.fit_one_cycle(10, 1e-2)\n",
    "mv_clf.export(\"mv_clf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

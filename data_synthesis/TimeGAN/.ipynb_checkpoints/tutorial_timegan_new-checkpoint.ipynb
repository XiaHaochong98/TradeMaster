{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ.get(\"tf_upgrade_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tf_upgrade_v2 \\\n",
    "#   --infile /home/zcxia/TradeMaster/data_synthesis/TimeGAN/timegan.py \\\n",
    "#   --outfile /home/zcxia/TradeMaster/data_synthesis/TimeGAN/timegan_v2.py \\\n",
    "#   --reportfile report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ.get(\"tf_upgrade_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeGAN Tutorial\n",
    "\n",
    "## Time-series Generative Adversarial Networks\n",
    "\n",
    "- Paper: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, \"Time-series Generative Adversarial Networks,\" Neural Information Processing Systems (NeurIPS), 2019.\n",
    "\n",
    "- Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks\n",
    "\n",
    "- Last updated Date: April 24th 2020\n",
    "\n",
    "- Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "\n",
    "This notebook describes the user-guide of a time-series synthetic data generation application using timeGAN framework. We use Stock, Energy, and Sine dataset as examples.\n",
    "\n",
    "### Prerequisite\n",
    "Clone https://github.com/jsyoon0823/timeGAN.git to the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary packages and functions call\n",
    "\n",
    "- timegan: Synthetic time-series data generation module\n",
    "- data_loading: 2 real datasets and 1 synthetic datasets loading and preprocessing\n",
    "- metrics: \n",
    "    - discriminative_metrics: classify real data from synthetic data\n",
    "    - predictive_metrics: train on synthetic, test on real\n",
    "    - visualization: PCA and tSNE analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary packages\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. TimeGAN model\n",
    "from timegan import timegan\n",
    "# 2. Data loading\n",
    "from data_loading import real_data_loading, sine_data_generation\n",
    "# 3. Metrics\n",
    "from metrics.discriminative_metrics import discriminative_score_metrics\n",
    "from metrics.predictive_metrics import predictive_score_metrics\n",
    "from metrics.visualization_metrics import visualization\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intervals(data):\n",
    "    index=data['index']\n",
    "    last_value=index[0]-1\n",
    "    last_index=0\n",
    "    intervals=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        if last_value!=index[i]-1:\n",
    "            intervals.append([last_index,i])\n",
    "            last_value=index[i]\n",
    "            last_index=i\n",
    "        last_value=index[i]\n",
    "    intervals.append([last_index, i])\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(data):\n",
    "    max_len=24\n",
    "    l=len(data)\n",
    "    to_fill=max_len-l\n",
    "    if to_fill!=0:\n",
    "        interval=max_len//to_fill\n",
    "        for j in range(to_fill):\n",
    "            idx=(interval+1)*j+interval\n",
    "            data.insert(min(idx,len(data)-1),float('nan'))\n",
    "    data=pd.Series(data).interpolate(method='polynomial', order=2)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    norm_data = numerator / (denominator + 1e-7)\n",
    "    return np.min(data, 0),np.max(data, 0),norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normlization(data):\n",
    "    normalized_data=(data-data.min())/(data.max()-data.min())\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_of_same_length(data,seq_len):\n",
    "    data_processed=[]\n",
    "    intervals=get_intervals(data)\n",
    "    temp_data=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "#         print(interval)\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:]\n",
    "        for i in range(0, len(data_seg) - seq_len):\n",
    "            _x = data_seg.iloc[i:i + seq_len,:]\n",
    "            temp_data.append(_x)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_prepareation(path):\n",
    "    data=pd.read_csv(path).reset_index()\n",
    "    tics=data['tic'].unique()\n",
    "    # features=[ 'open', 'high', 'low', 'close', 'adjcp','zopen', 'zhigh', 'zlow', 'zadjcp', 'zclose', 'zd_5', 'zd_10',\n",
    "    #    'zd_15', 'zd_20', 'zd_25', 'zd_30', 'pct_return', 'adjcp_filtered',\n",
    "    #    'pct_return_filtered','volume']\n",
    "    features=['open','high','low','close','adjcp','volume']\n",
    "    ret=[]\n",
    "    for col in data.columns:\n",
    "        if col in features:\n",
    "            ret.append(col)\n",
    "    features=ret\n",
    "    min_sclar_by_tic={}\n",
    "    max_sclar_by_tic={}\n",
    "    for tic in tics:\n",
    "        data_by_tic=data.loc[data['tic']==tic,features].astype(float)\n",
    "        min_scalr,max_sclar,norm_data_by_tic=MinMaxScaler(data_by_tic)\n",
    "        # print(min_scalr.shape)\n",
    "        data.loc[data['tic']==tic,features]=norm_data_by_tic\n",
    "        min_sclar_by_tic[tic]=min_scalr\n",
    "        max_sclar_by_tic[tic]=max_sclar\n",
    "    stock_group_num=len(data['stock_type'].unique())\n",
    "    regime_num=len(data['label'].unique())\n",
    "    for tic in tics:\n",
    "        with open('./data/scalr/'+str(tic)+'_minsclar'+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(min_sclar_by_tic[tic], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('./data/scalr/'+str(tic)+'_maxsclar'+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(max_sclar_by_tic[tic], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        for j in range(regime_num):\n",
    "            data_seg=data.loc[(data['tic']==tic) & (data['label']==j),['index','open','high','low','close','adjcp','volume']]\n",
    "    #         data_dict[(i,j)]=data_seg\n",
    "            data_seg.to_csv('./data/data_seg_'+tic+'_'+str(j)+'.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load original dataset and preprocess the loaded data.\n",
    "\n",
    "- data_name: stock, energy, or sine\n",
    "- seq_len: sequence length of the time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv('/home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/GOOG_labeled_3_24.csv').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv(\"./DJI_all_labeled_3_24.csv\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_prepareation(\"/home/hcxia/TradeMaster_dev/TradeMaster/data/data/other/DJI_all_labeled_3_24.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_prepareation('/home/hcxia/TradeMaster_dev/TradeMaster/data/data/other/GOOG_labeled_3_24.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data by stock group and regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize features with in own tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeGan Dataset description:\n",
    "Stocks. By contrast, sequences of stock prices are continuous-valued but aperiodic; furthermore,\n",
    "features are correlated with each other. We use the daily historical Google stocks data from 2004 to\n",
    "2019, including as features the volume and high, low, opening, closing, and adjusted closing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_trainning(path):\n",
    "    data=pd.read_csv(path).drop('index', axis=1)\n",
    "    data=data.reset_index().rename(columns={data.index.name:'index'})\n",
    "#     plt.plot(data['adjcp'])\n",
    "#     plt.show()\n",
    "#     display(data.head())\n",
    "    data.to_csv('temp_data.csv')\n",
    "    data=get_data_of_same_length(data,24)\n",
    "#     for i in range(20):\n",
    "#         plt.plot(data[i]['adjcp'])\n",
    "#         plt.show()\n",
    "#         print(i)\n",
    "#         display(data[i])\n",
    "#     display(len(data))\n",
    "    data=[d.loc[:,['open','high','low','close','adjcp','volume']].to_numpy() for d in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOG_data={}\n",
    "for i in range(3):\n",
    "    GOOG_data['data_seg_GOOG_'+str(i)]=prepare_data_for_trainning('./data/data_seg_GOOG_'+str(i)+'.csv')\n",
    "    print(i,len(GOOG_data['data_seg_GOOG_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/home/hcxia/TradeMaster_dev/TradeMaster/data/data/other/DJI_all_labeled_3_24.csv\").reset_index()\n",
    "tics=data['tic'].unique()\n",
    "data_dict_tic={}\n",
    "for tic in tics:\n",
    "    data_dict_tic[tic]={}\n",
    "    for i in range(3):\n",
    "        data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)]=prepare_data_for_trainning('./data/data_seg_'+str(tic)+'_'+str(i)+'.csv')\n",
    "        print(tic,i,len(data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/home/hcxia/TradeMaster_dev/TradeMaster/data/data/other/DJI_all_labeled_3_24.csv\").reset_index()\n",
    "# display(data.columns)\n",
    "tic_group_pair=data.loc[:,['tic','stock_type']]\n",
    "tic_group_pair=tic_group_pair.groupby(['tic','stock_type']).size().reset_index(name='Freq')\n",
    "stock_group_num=len(data['stock_type'].unique())\n",
    "tic_in_group={}\n",
    "for group in range(stock_group_num):\n",
    "#     if group not in tic_in_groupï¼š\n",
    "#         tic_in_group[group]=[]\n",
    "    tic_in_group[group]=list(tic_group_pair.loc[tic_group_pair['stock_type']==group,:]['tic'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tic_in_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/home/hcxia/TradeMaster_dev/TradeMaster/data/data/other/DJI_all_labeled_3_24.csv\").reset_index()\n",
    "stock_group_num=len(data['stock_type'].unique())\n",
    "data_dict_group={}\n",
    "for group in range(stock_group_num):\n",
    "    data_dict_group[group]={}\n",
    "    for i in range(3):\n",
    "        if 'data_seg_'+str(group)+'_'+str(i) not in data_dict_group[group]:\n",
    "            data_dict_group[group]['data_seg_'+str(group)+'_'+str(i)]=[]\n",
    "        for tic in tic_in_group[group]:\n",
    "            data_dict_group[group]['data_seg_'+str(group)+'_'+str(i)].extend(data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)])\n",
    "        print(group,i,len(data_dict_group[group]['data_seg_'+str(group)+'_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All dji stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all={}\n",
    "for i in range(3):\n",
    "    if 'data_seg_'+'all'+'_'+str(i) not in data_all:\n",
    "        data_all['data_seg_'+'all'+'_'+str(i)]=[]\n",
    "    for tic in tics:\n",
    "        data_all['data_seg_'+'all'+'_'+str(i)].extend(data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)])\n",
    "    print(i,len(data_all['data_seg_'+'all'+'_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data set:\n",
    "\n",
    "- GOOG_data\n",
    "- data_dict_tic (dict of dict by tic)\n",
    "- data_dict_group (dict of dict by group num)\n",
    "- data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set network parameters\n",
    "\n",
    "TimeGAN network parameters should be optimized for different datasets.\n",
    "\n",
    "- module: gru, lstm, or lstmLN\n",
    "- hidden_dim: hidden dimensions\n",
    "- num_layer: number of layers\n",
    "- iteration: number of training iterations\n",
    "- batch_size: the number of samples in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GOOG_data.keys())\n",
    "print(data_dict_tic.keys())\n",
    "print(data_dict_group.keys())\n",
    "print(data_all.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(data_set_dict):\n",
    "    parameters = dict()\n",
    "    parameters['module'] = 'gru' \n",
    "    parameters['hidden_dim'] = 24\n",
    "    parameters['num_layer'] = 3\n",
    "    parameters['iterations'] = 10000\n",
    "    parameters['batch_size'] = 128\n",
    "    gnerated_dataset_dict={}\n",
    "    for dataset_name, data_set in data_set_dict.items():\n",
    "        print(str(dataset_name))\n",
    "        gnerated_dataset_dict[dataset_name] = timegan(data_set, parameters,device=1,save_name=str(dataset_name))   \n",
    "    return gnerated_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_generated={}\n",
    "for tic in data_dict_tic.keys():\n",
    "    if tic not in data_dict_tic_generated:\n",
    "        data_dict_tic_generated[tic]=training(data_dict_tic[tic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_APPL_generated=training(data_dict_tic['AAPL'])\n",
    "data_dict_group_3_generated=training(data_dict_group[3])\n",
    "GOOG_data_generated=training(GOOG_data)\n",
    "data_all_generated=training(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_group_4_generated=training(data_dict_group[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save=[\n",
    "data_dict_tic_APPL_generated,\n",
    "data_dict_group_3_generated,\n",
    "GOOG_data_generated,\n",
    "data_all_generated,\n",
    "data_dict_group_4_generated\n",
    "]\n",
    "for tic in data_dict_tic_generated.keys():\n",
    "    locals()['data_dict_tic_'+tic+'_generated']=data_dict_tic_generated[tic]\n",
    "    data_to_save.append(locals()['data_dict_tic_'+tic+'_generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inspect\n",
    "\n",
    "\n",
    "def retrieve_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "for i in range(len(data_to_save)):\n",
    "    try:\n",
    "        with open('./generated_data/'+retrieve_name(data_to_save[i])[-1]+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(data_to_save[i], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('writen to '+retrieve_name(data_to_save[i])[-1]+'.pickle')\n",
    "    except:\n",
    "        print('skip this')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the generated data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_score(ori_data, generated_data):\n",
    "    metric_iteration = 5\n",
    "    discriminative_score = list()\n",
    "    for _ in range(metric_iteration):\n",
    "        temp_disc = discriminative_score_metrics(ori_data, generated_data)\n",
    "        discriminative_score.append(temp_disc)\n",
    "    print('Discriminative score: ' + str(np.round(np.mean(discriminative_score), 4)))\n",
    "    return np.round(np.mean(discriminative_score), 4)\n",
    "\n",
    "def pred_score(ori_data, generated_data):\n",
    "    predictive_score = list()\n",
    "    metric_iteration = 5\n",
    "    for tt in range(metric_iteration):\n",
    "        temp_pred = predictive_score_metrics(ori_data, generated_data)\n",
    "        predictive_score.append(temp_pred)   \n",
    "    print('Predictive score: ' + str(np.round(np.mean(predictive_score), 4)))\n",
    "    return np.round(np.mean(predictive_score), 4)\n",
    "    \n",
    "def evaluation(data_set_dict,genrated_data_set_dict):\n",
    "    length_dict={}\n",
    "    discriminative_score_dict={}\n",
    "    predictive_score_dict={}\n",
    "    for dataset_name, data_set in data_set_dict.items():\n",
    "        length_dict[dataset_name]=len(data_set_dict[dataset_name])\n",
    "        discriminative_score_dict[dataset_name] = dis_score(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name])\n",
    "        predictive_score_dict[dataset_name] = pred_score(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name])\n",
    "        visualization(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name], 'pca')\n",
    "        visualization(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name], 'tsne')\n",
    "    return length_dict,discriminative_score_dict,predictive_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_res={}\n",
    "for tic in data_dict_tic.keys():\n",
    "    data_dict_tic_res[tic]=[evaluation(data_dict_tic[tic],data_dict_tic_generated[tic])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_APPL_generated=training(data_dict_tic['AAPL'])\n",
    "data_dict_group_3_generated=training(data_dict_group[3])\n",
    "GOOG_data_generated=training(GOOG_data)\n",
    "data_all_generated=training(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_res=[evaluation(data_dict_tic['AAPL'],data_dict_tic_APPL_generated)]\n",
    "group_3_res=[evaluation(data_dict_group[3],data_dict_group_3_generated)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOG_data_res=[evaluation(GOOG_data,GOOG_data_generated)]\n",
    "all_data_res=[evaluation(data_all,data_all_generated)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_4_res=[evaluation(data_dict_group[4],data_dict_group_4_generated)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(GOOG_data_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(AAPL_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(group_3_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_data_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(group_4_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_res={}\n",
    "for tic in data_dict_tic_res.keys():\n",
    "    display(data_dict_tic_res[tic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(GOOG_processed_data,GOOG_genrated_data, 'pca')\n",
    "visualization(GOOG_processed_data,GOOG_genrated_data, 'tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in data_dict.keys():\n",
    "    print('vis of '+k)\n",
    "    visualization(data_dict[k], generated_data_dict[k], 'pca')\n",
    "    visualization(data_dict[k], generated_data_dict[k], 'tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leverage Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].pct_return.to_numpy()\n",
    "        std=data_seg.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tic in tics:\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+tic+'_'+str(j)+'.csv')\n",
    "        print('stock',tic,'regime',j)\n",
    "        std_list=get_std_list(data)\n",
    "        display(pd.DataFrame(std_list).describe()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by stock group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv')\n",
    "        print('stock',i,'regime',j)\n",
    "        std_list=get_std_list(data)\n",
    "        display(pd.DataFrame(std_list).describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].to_numpy()\n",
    "        std=data_seg.adj.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# key takeaway\n",
    "1. regime 0 has high variance mean and high variance variance\n",
    "2. regime 1 has low variance mean and low variance variance\n",
    "3. regime 2 has low variance mean and low/high variance variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train Static learning classification discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sktime.classification.kernel_based import RocketClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_of_same_length_df(data,seq_len):\n",
    "    data_processed=[]\n",
    "    intervals=get_intervals(data)\n",
    "    temp_data=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:]\n",
    "        for i in range(0, len(data_seg) - seq_len):\n",
    "            _x = data[i:i + seq_len]\n",
    "            temp_data.append(_x)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data_seg_'+\"0\"+'_'+\"0\"+'.csv')\n",
    "display(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tic in tics:\n",
    "    print(tic)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+tic+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length_df(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend(process_data)\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    classifier = RocketClassifier(num_kernels=2000,n_jobs=-1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL single stock classification have unbelieve 100% acc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    print('stock_group',i)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length_df(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend(process_data)\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    classifier = RocketClassifier(num_kernels=2000,n_jobs=-1,use_multivariate='yes')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still 1.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# classifier = TimeSeriesForestClassifier()\n",
    "# classifier.fit(X_train, y_train)\n",
    "# y_pred = classifier.predict(X_test)\n",
    "# accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train Deep learning classification discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tsai.all import *\n",
    "my_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    print('stock_group',i)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend([p.transpose() for p in process_data])\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X=np.array(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    X, y, splits = combine_split_data([X_train, X_test], [y_train, y_test])\n",
    "    tfms  = [None, [Categorize()]]\n",
    "    dsets = TSDatasets(X, y, tfms=tfms, splits=splits, inplace=True)\n",
    "    dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=[TSStandardize()], num_workers=0)\n",
    "    model = InceptionTime(dls.vars, dls.c)\n",
    "    learn = Learner(dls, model, metrics=accuracy)\n",
    "    learn.fit_one_cycle(25, lr_max=1e-3)\n",
    "    learn.plot_metrics()\n",
    "    learn.save_all(path='export', dls_fname='dls_'+str(i)+'_'+str(j), model_fname='model_'+str(i)+'_'+str(j), learner_fname='learner_'+str(i)+'_'+str(j))\n",
    "#     display(type(X_train),X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key takeaway\n",
    "\n",
    "InceptionTime can do the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, splits = get_classification_data('LSST', split_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms  = [None, TSClassification()] # TSClassification == Categorize\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, new_y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=[64, 128])\n",
    "dls.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].to_numpy()\n",
    "        std=data_seg.adj.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = TSStandardize(by_sample=True)\n",
    "mv_clf = TSClassifier(X, y, splits=splits, path='models', arch=InceptionTimePlus, batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph())\n",
    "mv_clf.fit_one_cycle(10, 1e-2)\n",
    "mv_clf.export(\"mv_clf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ.get(\"tf_upgrade_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tf_upgrade_v2 \\\n",
    "#   --infile /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/timegan.py \\\n",
    "#   --outfile /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/timegan_v2_new.py \\\n",
    "#   --reportfile report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ.get(\"tf_upgrade_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeGAN Tutorial\n",
    "\n",
    "## Time-series Generative Adversarial Networks\n",
    "\n",
    "- Paper: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, \"Time-series Generative Adversarial Networks,\" Neural Information Processing Systems (NeurIPS), 2019.\n",
    "\n",
    "- Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks\n",
    "\n",
    "- Last updated Date: April 24th 2020\n",
    "\n",
    "- Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "\n",
    "This notebook describes the user-guide of a time-series synthetic data generation application using timeGAN framework. We use Stock, Energy, and Sine dataset as examples.\n",
    "\n",
    "### Prerequisite\n",
    "Clone https://github.com/jsyoon0823/timeGAN.git to the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary packages and functions call\n",
    "\n",
    "- timegan: Synthetic time-series data generation module\n",
    "- data_loading: 2 real datasets and 1 synthetic datasets loading and preprocessing\n",
    "- metrics: \n",
    "    - discriminative_metrics: classify real data from synthetic data\n",
    "    - predictive_metrics: train on synthetic, test on real\n",
    "    - visualization: PCA and tSNE analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary packages\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. TimeGAN model\n",
    "from timegan import timegan\n",
    "# 2. Data loading\n",
    "from data_loading import real_data_loading, sine_data_generation\n",
    "# 3. Metrics\n",
    "from metrics.discriminative_metrics import discriminative_score_metrics\n",
    "from metrics.predictive_metrics import predictive_score_metrics\n",
    "from metrics.visualization_metrics import visualization\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intervals(data):\n",
    "    index=data['index']\n",
    "    last_value=index[0]-1\n",
    "    last_index=0\n",
    "    intervals=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        if last_value!=index[i]-1:\n",
    "            intervals.append([last_index,i])\n",
    "            last_value=index[i]\n",
    "            last_index=i\n",
    "        last_value=index[i]\n",
    "    intervals.append([last_index, i])\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(data):\n",
    "    max_len=24\n",
    "    l=len(data)\n",
    "    to_fill=max_len-l\n",
    "    if to_fill!=0:\n",
    "        interval=max_len//to_fill\n",
    "        for j in range(to_fill):\n",
    "            idx=(interval+1)*j+interval\n",
    "            data.insert(min(idx,len(data)-1),float('nan'))\n",
    "    data=pd.Series(data).interpolate(method='polynomial', order=2)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    norm_data = numerator / (denominator + 1e-7)\n",
    "    return np.min(data, 0),np.max(data, 0),norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normlization(data):\n",
    "    normalized_data=(data-data.min())/(data.max()-data.min())\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_of_same_length(data,seq_len):\n",
    "    data_processed=[]\n",
    "    intervals=get_intervals(data)\n",
    "    temp_data=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "#         print(interval)\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:]\n",
    "        for i in range(0, len(data_seg) - seq_len):\n",
    "            _x = data_seg.iloc[i:i + seq_len,:]\n",
    "            temp_data.append(_x)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_prepareation(path):\n",
    "    data=pd.read_csv(path).reset_index()\n",
    "    tics=data['tic'].unique()\n",
    "    # features=[ 'open', 'high', 'low', 'close', 'adjcp','zopen', 'zhigh', 'zlow', 'zadjcp', 'zclose', 'zd_5', 'zd_10',\n",
    "    #    'zd_15', 'zd_20', 'zd_25', 'zd_30', 'pct_return', 'adjcp_filtered',\n",
    "    #    'pct_return_filtered','volume']\n",
    "    features=['open','high','low','close','adjcp','volume']\n",
    "    ret=[]\n",
    "    for col in data.columns:\n",
    "        if col in features:\n",
    "            ret.append(col)\n",
    "    features=ret\n",
    "    min_sclar_by_tic={}\n",
    "    max_sclar_by_tic={}\n",
    "    for tic in tics:\n",
    "        data_by_tic=data.loc[data['tic']==tic,features].astype(float)\n",
    "        min_scalr,max_sclar,norm_data_by_tic=MinMaxScaler(data_by_tic)\n",
    "        # print(min_scalr.shape)\n",
    "        data.loc[data['tic']==tic,features]=norm_data_by_tic\n",
    "        min_sclar_by_tic[tic]=min_scalr\n",
    "        max_sclar_by_tic[tic]=max_sclar\n",
    "    stock_group_num=len(data['stock_type'].unique())\n",
    "    regime_num=len(data['label'].unique())\n",
    "    for tic in tics:\n",
    "        with open('./data/scalr/'+str(tic)+'_minsclar'+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(min_sclar_by_tic[tic], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('./data/scalr/'+str(tic)+'_maxsclar'+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(max_sclar_by_tic[tic], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        for j in range(regime_num):\n",
    "            data_seg=data.loc[(data['tic']==tic) & (data['label']==j),['index','open','high','low','close','adjcp','volume']]\n",
    "    #         data_dict[(i,j)]=data_seg\n",
    "            data_seg.to_csv('./data/data_seg_'+tic+'_'+str(j)+'.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load original dataset and preprocess the loaded data.\n",
    "\n",
    "- data_name: stock, energy, or sine\n",
    "- seq_len: sequence length of the time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_acl18_ori_path='/data/home/zwt/mae_st_rebuild/data/acl18_inter/features/'\n",
    "mae_acl18_generated_path='/home/hcxia/mae_st_reframe_pretrained_eval/mae_st_reframe/pretrained_simmim/output_dir/exp05_patchsize1x1/simmim_pretrain/vit_maskratio05/generated_data/epoch_1600/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pathlist = Path(mae_acl18_ori_path).rglob(\"*\")\n",
    "mae_acl18_inter_ori_dict={}\n",
    "mae_acl18_inter_ori_dict['all']=[]\n",
    "mae_acl18_inter_ori_small_dict={}\n",
    "mae_acl18_inter_ori_small_dict['all']=[]\n",
    "features = [\n",
    "    'open',\n",
    "    'high',\n",
    "    'low',\n",
    "    'close',\n",
    "    'kmid2',\n",
    "    'kup2',\n",
    "    'klow',\n",
    "    'klow2',\n",
    "    'ksft2',\n",
    "    'roc_5',\n",
    "    'roc_10',\n",
    "    'roc_20',\n",
    "    'roc_30',\n",
    "    'roc_60',\n",
    "    'ma_5',\n",
    "    'ma_10',\n",
    "    'ma_20',\n",
    "    'ma_30',\n",
    "    'ma_60',\n",
    "    'std_5',\n",
    "    'std_10',\n",
    "    'std_20',\n",
    "    'std_30',\n",
    "    'std_60',\n",
    "    'beta_5',\n",
    "    'beta_10',\n",
    "    'beta_20',\n",
    "    'beta_30',\n",
    "    'beta_60',\n",
    "    'max_5',\n",
    "    'max_10',\n",
    "    'max_20',\n",
    "    'max_30',\n",
    "    'max_60',\n",
    "    'min_5',\n",
    "    'min_10',\n",
    "    'min_20',\n",
    "    'min_30',\n",
    "    'min_60',\n",
    "    'qtlu_5',\n",
    "    'qtlu_10',\n",
    "    'qtlu_20',\n",
    "    'qtlu_30',\n",
    "    'qtlu_60',\n",
    "    'qtld_5',\n",
    "    'qtld_10',\n",
    "    'qtld_20',\n",
    "    'qtld_30',\n",
    "    'qtld_60',\n",
    "    'rank_5',\n",
    "    'rank_10',\n",
    "    'rank_20',\n",
    "    'rank_30',\n",
    "    'rank_60',\n",
    "    'imax_5',\n",
    "    'imax_10',\n",
    "    'imax_20',\n",
    "    'imax_30',\n",
    "    'imax_60',\n",
    "    'imin_5',\n",
    "    'imin_10',\n",
    "    'imin_20',\n",
    "    'imin_30',\n",
    "    'imin_60',\n",
    "    'imxd_5',\n",
    "    'imxd_10',\n",
    "    'imxd_20',\n",
    "    'imxd_30',\n",
    "    'imxd_60',\n",
    "    'cntp_5',\n",
    "    'cntp_10',\n",
    "    'cntp_20',\n",
    "    'cntp_30',\n",
    "    'cntp_60',\n",
    "    'cntn_5',\n",
    "    'cntn_10',\n",
    "    'cntn_20',\n",
    "    'cntn_30',\n",
    "    'cntn_60',\n",
    "    'cntd_5',\n",
    "    'cntd_10',\n",
    "    'cntd_20',\n",
    "    'cntd_30',\n",
    "    'cntd_60',\n",
    "    'sump_5',\n",
    "    'sump_10',\n",
    "    'sump_20',\n",
    "    'sump_30',\n",
    "    'sump_60',\n",
    "    'sumn_5',\n",
    "    'sumn_10',\n",
    "    'sumn_20',\n",
    "    'sumn_30',\n",
    "    'sumn_60',\n",
    "    'sumd_5',\n",
    "    'sumd_10',\n",
    "    'sumd_20',\n",
    "    'sumd_30',\n",
    "    'sumd_60',\n",
    "]\n",
    "seq_len=24\n",
    "for path in pathlist:\n",
    "    data=pd.read_csv(path).loc[:,features]\n",
    "    for i in range(0, len(data) - seq_len):\n",
    "            _x = data.iloc[i:i + seq_len,:].to_numpy()\n",
    "            _x_small = data.iloc[i:i + seq_len,:4].to_numpy()\n",
    "            mae_acl18_inter_ori_dict['all'].append(_x)\n",
    "            mae_acl18_inter_ori_small_dict['all'].append(_x_small)\n",
    "    # print(mae_acl18_inter_ori_small_dict['all'][-1].shape)\n",
    "\n",
    "display(len(mae_acl18_inter_ori_dict['all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_188545/623018721.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmae_acl18_inter_generated_small_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# print(mae_acl18_inter_generated_dict['all'][-1].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmae_acl18_inter_generated_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae_acl18_inter_generated_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmae_acl18_inter_generated_small_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae_acl18_inter_generated_small_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae_acl18_inter_generated_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TimeGan/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "pathlist = Path(mae_acl18_generated_path).rglob(\"*\")\n",
    "mae_acl18_inter_generated_dict={}\n",
    "mae_acl18_inter_generated_dict['all']=[]\n",
    "mae_acl18_inter_generated_small_dict={}\n",
    "mae_acl18_inter_generated_small_dict['all']=[]\n",
    "for path in pathlist:\n",
    "    print(path)\n",
    "    data=np.load(path)\n",
    "    mae_acl18_inter_generated_dict['all'].append(data)\n",
    "    mae_acl18_inter_generated_small_dict['all'].append(data[:,:4])\n",
    "    # print(mae_acl18_inter_generated_dict['all'][-1].shape)\n",
    "mae_acl18_inter_generated_dict['all']=np.stack(mae_acl18_inter_generated_dict['all'],axis=0)\n",
    "mae_acl18_inter_generated_small_dict['all']=np.stack(mae_acl18_inter_generated_small_dict['all'],axis=0)\n",
    "display(mae_acl18_inter_generated_dict['all'].shape)\n",
    "display(mae_acl18_inter_generated_small_dict['all'].shape)\n",
    "    # print(data.shape)\n",
    "    # print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data by stock group and regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize features with in own tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_trainning(path,label,label_number=3):\n",
    "    data=pd.read_csv(path).drop('index', axis=1)\n",
    "    data=data.reset_index().rename(columns={data.index.name:'index'})\n",
    "#     plt.plot(data['adjcp'])\n",
    "#     plt.show()\n",
    "#     display(data.head())\n",
    "    data.to_csv('temp_data.csv')\n",
    "    data=get_data_of_same_length(data,24)\n",
    "#     for i in range(20):\n",
    "#         plt.plot(data[i]['adjcp'])\n",
    "#         plt.show()\n",
    "#         print(i)\n",
    "#         display(data[i])\n",
    "#     display(len(data))\n",
    "    data_res=[]\n",
    "    label_res=[]\n",
    "    for d in data:\n",
    "        data_res.append(d.loc[:,['open','high','low','close','adjcp','volume']].to_numpy())\n",
    "        label_temp=np.zeros(label_number)\n",
    "        label_temp[label]=1\n",
    "        label_res.append(label_temp)\n",
    "    return data_res,label_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 673\n",
      "1 1591\n",
      "2 1529\n"
     ]
    }
   ],
   "source": [
    "GOOG_data={}\n",
    "GOOG_data_label={}\n",
    "for i in range(3):\n",
    "    GOOG_data['data_seg_GOOG_'+str(i)],GOOG_data_label['data_seg_GOOG_'+str(i)]=prepare_data_for_trainning('./data/data_seg_GOOG_'+str(i)+'.csv',i)\n",
    "    print(i,len(GOOG_data['data_seg_GOOG_'+str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data_seg_GOOG_0', 'data_seg_GOOG_1', 'data_seg_GOOG_2'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOOG_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 6)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOOG_data['data_seg_GOOG_0'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/home/hcxia/TradeMaster_dev/TradeMaster/data/data/other/DJI_all_labeled_3_24.csv\").reset_index()\n",
    "tics=data['tic'].unique()\n",
    "data_dict_tic={}\n",
    "data_dict_tic_label={}\n",
    "for tic in tics:\n",
    "    data_dict_tic[tic]={}\n",
    "    data_dict_tic_label[tic]={}\n",
    "    for i in range(3):\n",
    "        data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)],data_dict_tic_label[tic]['data_seg_'+str(tic)+'_'+str(i)]=prepare_data_for_trainning('./data/data_seg_'+str(tic)+'_'+str(i)+'.csv',i)\n",
    "        print(tic,i,len(data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/home/hcxia/TradeMaster_dev/TradeMaster/data/data/other/DJI_all_labeled_3_24.csv\").reset_index()\n",
    "# display(data.columns)\n",
    "tic_group_pair=data.loc[:,['tic','stock_type']]\n",
    "tic_group_pair=tic_group_pair.groupby(['tic','stock_type']).size().reset_index(name='Freq')\n",
    "stock_group_num=len(data['stock_type'].unique())\n",
    "tic_in_group={}\n",
    "for group in range(stock_group_num):\n",
    "#     if group not in tic_in_groupï¼š\n",
    "#         tic_in_group[group]=[]\n",
    "    tic_in_group[group]=list(tic_group_pair.loc[tic_group_pair['stock_type']==group,:]['tic'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tic_in_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/home/hcxia/TradeMaster_dev/TradeMaster/data/data/other/DJI_all_labeled_3_24.csv\").reset_index()\n",
    "stock_group_num=len(data['stock_type'].unique())\n",
    "data_dict_group={}\n",
    "data_dict_group_label={}\n",
    "for group in range(stock_group_num):\n",
    "    data_dict_group[group]={}\n",
    "    data_dict_group_label[group]={}\n",
    "    for i in range(3):\n",
    "        if 'data_seg_'+str(group)+'_'+str(i) not in data_dict_group[group]:\n",
    "            data_dict_group[group]['data_seg_'+str(group)+'_'+str(i)]=[]\n",
    "            data_dict_group_label[group]['data_seg_'+str(group)+'_'+str(i)]=[]\n",
    "        for tic in tic_in_group[group]:\n",
    "            data_dict_group[group]['data_seg_'+str(group)+'_'+str(i)].extend(data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)])\n",
    "            data_dict_group_label[group]['data_seg_'+str(group)+'_'+str(i)].extend(data_dict_tic_label[tic]['data_seg_'+str(tic)+'_'+str(i)])\n",
    "        print(group,i,len(data_dict_group[group]['data_seg_'+str(group)+'_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All dji stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all={}\n",
    "data_all_label={}\n",
    "for i in range(3):\n",
    "    if 'data_seg_'+'all'+'_'+str(i) not in data_all:\n",
    "        data_all['data_seg_'+'all'+'_'+str(i)]=[]\n",
    "        data_all_label['data_seg_'+'all'+'_'+str(i)]=[]\n",
    "    for tic in tics:\n",
    "        data_all['data_seg_'+'all'+'_'+str(i)].extend(data_dict_tic[tic]['data_seg_'+str(tic)+'_'+str(i)])\n",
    "        data_all_label['data_seg_'+'all'+'_'+str(i)].extend(data_dict_tic_label[tic]['data_seg_'+str(tic)+'_'+str(i)])\n",
    "    print(i,len(data_all['data_seg_'+'all'+'_'+str(i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_training_data=[]\n",
    "style_training_label=[]\n",
    "for i in range(3):\n",
    "    style_training_data.extend(data_all['data_seg_'+'all'+'_'+str(i)])\n",
    "    style_training_label.extend(data_all_label['data_seg_'+'all'+'_'+str(i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data set:\n",
    "\n",
    "- GOOG_data\n",
    "- data_dict_tic (dict of dict by tic)\n",
    "- data_dict_group (dict of dict by group num)\n",
    "- data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set network parameters\n",
    "\n",
    "TimeGAN network parameters should be optimized for different datasets.\n",
    "\n",
    "- module: gru, lstm, or lstmLN\n",
    "- hidden_dim: hidden dimensions\n",
    "- num_layer: number of layers\n",
    "- iteration: number of training iterations\n",
    "- batch_size: the number of samples in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GOOG_data.keys())\n",
    "print(data_dict_tic.keys())\n",
    "print(data_dict_group.keys())\n",
    "print(data_all.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(data_set_dict,label_set_dict,style_training_data,style_training_label,style_training=True,only_style_training=False,from_join_training=False):\n",
    "    parameters = dict()\n",
    "    parameters['module'] = 'gru' \n",
    "    parameters['hidden_dim'] = 24\n",
    "    parameters['num_layer'] = 3\n",
    "    parameters['iterations'] = 10000\n",
    "    parameters['batch_size'] = 128\n",
    "    gnerated_dataset_dict={}\n",
    "    for dataset_name, data_set in data_set_dict.items():\n",
    "        print(str(dataset_name))\n",
    "        label=int(dataset_name[-1])\n",
    "        print(label)\n",
    "        # styletimegan(ori_data, parameters,training_label,nb_classes,label,style_training_data,style_training=True,only_style_training=False,save_name=None,from_join_training=False):\n",
    "        try:\n",
    "            gnerated_dataset_dict[dataset_name] = styletimegan(ori_data=data_set,\n",
    "                                                               parameters=parameters,\n",
    "                                                               style_training_label=style_training_label,\n",
    "                                                               save_name=str(dataset_name),\n",
    "                                                               label=label_set_dict[dataset_name],\n",
    "                                                              nb_classes=3,\n",
    "                                                              style_training_data=style_training_data,\n",
    "                                                               style_training=style_training,\n",
    "                                                              only_style_training=only_style_training,\n",
    "                                                               from_join_training=from_join_training\n",
    "                                                              )\n",
    "        except:\n",
    "            gnerated_dataset_dict[dataset_name] = styletimegan(ori_data=data_set,\n",
    "                                                               parameters=parameters,\n",
    "                                                               style_training_label=style_training_label,\n",
    "                                                               save_name=str(dataset_name),\n",
    "                                                               label=label_set_dict[dataset_name],\n",
    "                                                              nb_classes=3,\n",
    "                                                              style_training_data=style_training_data,\n",
    "                                                               style_training=style_training,\n",
    "                                                              only_style_training=False,\n",
    "                                                               from_join_training=False\n",
    "                                                              )\n",
    "        if only_style_training:\n",
    "            break\n",
    "    return gnerated_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "# importlib.reload(styletimegan)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOG_data_generated=training(data_set_dict=GOOG_data,\n",
    "                             label_set_dict=GOOG_data_label,\n",
    "                             style_training_data=style_training_data,\n",
    "                             style_training_label=style_training_label,\n",
    "                             style_training=True,\n",
    "                             only_style_training=True,\n",
    "                             from_join_training=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOG_data_generated=training(data_set_dict=GOOG_data,\n",
    "                             label_set_dict=GOOG_data_label,\n",
    "                             style_training_data=style_training_data,\n",
    "                             style_training_label=style_training_label,\n",
    "                             style_training=False,\n",
    "                             only_style_training=False,\n",
    "                             from_join_training=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_generated={}\n",
    "for tic in data_dict_tic.keys():\n",
    "    if tic not in data_dict_tic_generated:\n",
    "        data_dict_tic_generated[tic]=training(data_dict_tic[tic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_APPL_generated=training(data_dict_tic['AAPL'])\n",
    "data_dict_group_3_generated=training(data_dict_group[3])\n",
    "GOOG_data_generated=training(GOOG_data)\n",
    "data_all_generated=training(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_group_4_generated=training(data_dict_group[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save=[\n",
    "data_dict_tic_APPL_generated,\n",
    "data_dict_group_3_generated,\n",
    "GOOG_data_generated,\n",
    "data_all_generated,\n",
    "data_dict_group_4_generated\n",
    "]\n",
    "for tic in data_dict_tic_generated.keys():\n",
    "    locals()['data_dict_tic_'+tic+'_generated']=data_dict_tic_generated[tic]\n",
    "    data_to_save.append(locals()['data_dict_tic_'+tic+'_generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inspect\n",
    "\n",
    "\n",
    "def retrieve_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "for i in range(len(data_to_save)):\n",
    "    try:\n",
    "        with open('./generated_data/'+retrieve_name(data_to_save[i])[-1]+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(data_to_save[i], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('writen to '+retrieve_name(data_to_save[i])[-1]+'.pickle')\n",
    "    except:\n",
    "        print('skip this')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the generated data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_score(ori_data, generated_data):\n",
    "    metric_iteration = 5\n",
    "    discriminative_score = list()\n",
    "    for _ in range(metric_iteration):\n",
    "        temp_disc = discriminative_score_metrics(ori_data, generated_data)\n",
    "        discriminative_score.append(temp_disc)\n",
    "    print('Discriminative score: ' + str(np.round(np.mean(discriminative_score), 4)))\n",
    "    return np.round(np.mean(discriminative_score), 4)\n",
    "\n",
    "def pred_score(ori_data, generated_data):\n",
    "    predictive_score = list()\n",
    "    metric_iteration = 5\n",
    "    for tt in range(metric_iteration):\n",
    "        temp_pred = predictive_score_metrics(ori_data, generated_data)\n",
    "        predictive_score.append(temp_pred)   \n",
    "    print('Predictive score: ' + str(np.round(np.mean(predictive_score), 4)))\n",
    "    return np.round(np.mean(predictive_score), 4)\n",
    "    \n",
    "def evaluation(data_set_dict,genrated_data_set_dict):\n",
    "    length_dict={}\n",
    "    discriminative_score_dict={}\n",
    "    predictive_score_dict={}\n",
    "    for dataset_name, data_set in data_set_dict.items():\n",
    "        length_dict[dataset_name]=len(data_set_dict[dataset_name])\n",
    "        # print(len(data_set_dict[dataset_name]),len(genrated_data_set_dict[dataset_name]))\n",
    "        discriminative_score_dict[dataset_name] = dis_score(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name])\n",
    "        predictive_score_dict[dataset_name] = pred_score(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name])\n",
    "        visualization(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name], 'pca')\n",
    "        visualization(data_set_dict[dataset_name],genrated_data_set_dict[dataset_name], 'tsne')\n",
    "    return length_dict,discriminative_score_dict,predictive_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 99)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_acl18_inter_ori_dict['all'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_188545/159280612.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmae_acl18_inter_generated_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "mae_acl18_inter_generated_dict['all'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:40:53.195254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 12:40:53.195279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 12:41:07.715725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 12:41:07.715747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 12:41:22.789748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 12:41:22.789767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 12:41:37.487848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 12:41:37.487872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 12:41:52.410185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 12:41:52.410204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminative score: 0.4418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:42:06.595275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 12:42:06.595296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 12:42:42.857450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 12:42:42.857470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 12:43:21.046542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 12:43:21.046565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 12:43:58.935715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 12:43:58.935741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 12:44:36.431316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 12:44:36.431337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive score: 84.4414\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 69738 is out of bounds for axis 0 with size 68829",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2940566/1890940326.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmae_acl18_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae_acl18_inter_ori_small_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmae_acl18_inter_generated_small_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2940566/1672263469.py\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(data_set_dict, genrated_data_set_dict)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdiscriminative_score_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpredictive_score_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mvisualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pca'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mvisualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tsne'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlength_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscriminative_score_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictive_score_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/visualization_metrics.py\u001b[0m in \u001b[0;36mvisualization\u001b[0;34m(ori_data, generated_data, analysis, savefig)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mori_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mori_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0mgenerated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mori_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 69738 is out of bounds for axis 0 with size 68829"
     ]
    }
   ],
   "source": [
    "mae_acl18_res=evaluation(mae_acl18_inter_ori_small_dict,mae_acl18_inter_generated_small_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:39: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:57: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:76: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:76: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:77: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:78: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/hcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/hcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/hcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/hcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/anaconda3/envs/TimeGan/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:81: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:96: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:100: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 23:58:40.575652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-19 23:58:40.575787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 23:58:40.762626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: NVIDIA RTX A6000 major: 8 minor: 6 memoryClockRate(GHz): 1.8\n",
      "pciBusID: 0000:21:00.0\n",
      "2023-01-19 23:58:40.762760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 23:58:40.763208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \n",
      "name: NVIDIA RTX A6000 major: 8 minor: 6 memoryClockRate(GHz): 1.8\n",
      "pciBusID: 0000:22:00.0\n",
      "2023-01-19 23:58:40.763256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 23:58:40.763420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: \n",
      "name: NVIDIA RTX A6000 major: 8 minor: 6 memoryClockRate(GHz): 1.8\n",
      "pciBusID: 0000:41:00.0\n",
      "2023-01-19 23:58:40.763459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 23:58:40.763616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: \n",
      "name: NVIDIA RTX A6000 major: 8 minor: 6 memoryClockRate(GHz): 1.8\n",
      "pciBusID: 0000:43:00.0\n",
      "2023-01-19 23:58:40.763740: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-19 23:58:40.763784: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-19 23:58:40.763819: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-19 23:58:40.763852: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-19 23:58:40.763885: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-19 23:58:40.763918: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\n",
      "2023-01-19 23:58:40.763951: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-19 23:58:40.763955: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-01-19 23:58:40.764779: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2023-01-19 23:58:40.791134: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2694945000 Hz\n",
      "2023-01-19 23:58:40.794999: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3637b2d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-19 23:58:40.795013: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/discriminative_metrics.py:101: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 23:58:41.133118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 23:58:41.133296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 23:58:41.133487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 23:58:41.212799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 23:58:41.213272: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x93d4df0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-19 23:58:41.213286: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-01-19 23:58:41.213290: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-01-19 23:58:41.213293: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-01-19 23:58:41.213296: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-01-19 23:58:41.213530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-19 23:58:41.213537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-19 23:59:17.655574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-19 23:59:17.655593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-19 23:59:53.632171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-19 23:59:53.632193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 00:00:29.284409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 00:00:29.284429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 00:01:05.354438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 00:01:05.354463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminative score: 0.5\n",
      "WARNING:tensorflow:From /home/hcxia/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/predictive_metrics.py:81: The name tf.losses.absolute_difference is deprecated. Please use tf.compat.v1.losses.absolute_difference instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 00:01:41.056516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 00:01:41.056539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 00:03:11.415545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 00:03:11.415567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 00:04:40.386127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 00:04:40.386150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 00:06:10.330807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 00:06:10.330827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "2023-01-20 00:07:40.905439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-20 00:07:40.905464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive score: 0.1345\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 73284 is out of bounds for axis 0 with size 68829",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2940566/2233474832.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmae_acl18_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae_acl18_inter_ori_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmae_acl18_inter_generated_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2940566/1672263469.py\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(data_set_dict, genrated_data_set_dict)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdiscriminative_score_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpredictive_score_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mvisualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pca'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mvisualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenrated_data_set_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tsne'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlength_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscriminative_score_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictive_score_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TradeMaster_dev/TradeMaster/data_synthesis/TimeGAN/metrics/visualization_metrics.py\u001b[0m in \u001b[0;36mvisualization\u001b[0;34m(ori_data, generated_data, analysis, savefig)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mori_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mori_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0mgenerated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mori_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 73284 is out of bounds for axis 0 with size 68829"
     ]
    }
   ],
   "source": [
    "mae_acl18_res=evaluation(mae_acl18_inter_ori_dict,mae_acl18_inter_generated_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(data_dict_tic[tic],data_dict_tic_generated[tic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_res={}\n",
    "for tic in data_dict_tic.keys():\n",
    "    if tic not in data_dict_tic_res:\n",
    "        data_dict_tic_res[tic]=[evaluation(data_dict_tic[tic],data_dict_tic_generated[tic])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict_tic_APPL_generated=training(data_dict_tic['AAPL'])\n",
    "# data_dict_group_3_generated=training(data_dict_group[3])\n",
    "# GOOG_data_generated=training(GOOG_data)\n",
    "# data_all_generated=training(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_res=[evaluation(data_dict_tic['AAPL'],data_dict_tic_APPL_generated)]\n",
    "group_3_res=[evaluation(data_dict_group[3],data_dict_group_3_generated)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data_seg_GOOG_0', 'data_seg_GOOG_1', 'data_seg_GOOG_2'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOOG_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 99)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_acl18_inter_ori_dict['all'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mae_acl18_inter_ori_dict['all'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GOOG_data['data_seg_GOOG_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(GOOG_data['data_seg_GOOG_0'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 6)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOOG_data['data_seg_GOOG_0'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GOOG_data_generated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2439661/2307315644.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGOOG_data_generated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_seg_GOOG_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'GOOG_data_generated' is not defined"
     ]
    }
   ],
   "source": [
    "GOOG_data_generated['data_seg_GOOG_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOG_data_res=[evaluation(GOOG_data,GOOG_data_generated)]\n",
    "all_data_res=[evaluation(data_all,data_all_generated)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_4_res=[evaluation(data_dict_group[4],data_dict_group_4_generated)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(GOOG_data_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(AAPL_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(group_3_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_data_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(group_4_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_tic_res={}\n",
    "for tic in data_dict_tic_res.keys():\n",
    "    display(data_dict_tic_res[tic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(GOOG_processed_data,GOOG_genrated_data, 'pca')\n",
    "visualization(GOOG_processed_data,GOOG_genrated_data, 'tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in data_dict.keys():\n",
    "    print('vis of '+k)\n",
    "    visualization(data_dict[k], generated_data_dict[k], 'pca')\n",
    "    visualization(data_dict[k], generated_data_dict[k], 'tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leverage Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].pct_return.to_numpy()\n",
    "        std=data_seg.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tic in tics:\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+tic+'_'+str(j)+'.csv')\n",
    "        print('stock',tic,'regime',j)\n",
    "        std_list=get_std_list(data)\n",
    "        display(pd.DataFrame(std_list).describe()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by stock group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv')\n",
    "        print('stock',i,'regime',j)\n",
    "        std_list=get_std_list(data)\n",
    "        display(pd.DataFrame(std_list).describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].to_numpy()\n",
    "        std=data_seg.adj.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# key takeaway\n",
    "1. regime 0 has high variance mean and high variance variance\n",
    "2. regime 1 has low variance mean and low variance variance\n",
    "3. regime 2 has low variance mean and low/high variance variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train Static learning classification discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sktime.classification.kernel_based import RocketClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_of_same_length_df(data,seq_len):\n",
    "    data_processed=[]\n",
    "    intervals=get_intervals(data)\n",
    "    temp_data=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:]\n",
    "        for i in range(0, len(data_seg) - seq_len):\n",
    "            _x = data[i:i + seq_len]\n",
    "            temp_data.append(_x)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data_seg_'+\"0\"+'_'+\"0\"+'.csv')\n",
    "display(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tic in tics:\n",
    "    print(tic)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+tic+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length_df(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend(process_data)\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    classifier = RocketClassifier(num_kernels=2000,n_jobs=-1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL single stock classification have unbelieve 100% acc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    print('stock_group',i)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length_df(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend(process_data)\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    classifier = RocketClassifier(num_kernels=2000,n_jobs=-1,use_multivariate='yes')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still 1.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# classifier = TimeSeriesForestClassifier()\n",
    "# classifier.fit(X_train, y_train)\n",
    "# y_pred = classifier.predict(X_test)\n",
    "# accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train Deep learning classification discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tsai.all import *\n",
    "my_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stock_group_num):\n",
    "    print('stock_group',i)\n",
    "    X=[]\n",
    "    y=np.empty(0)\n",
    "    for j in range(regime_num):\n",
    "        data=pd.read_csv('data_seg_'+str(i)+'_'+str(j)+'.csv').loc[:,['index', 'open', 'high', 'low', 'close', 'adjcp',\n",
    "       'pct_return', 'adjcp_filtered', 'pct_return_filtered']]\n",
    "        process_data=get_data_of_same_length(data,24)\n",
    "        label=np.full(len(process_data), j)\n",
    "        X.extend([p.transpose() for p in process_data])\n",
    "        y=np.concatenate((y, label), axis=0)\n",
    "    X=np.array(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    X, y, splits = combine_split_data([X_train, X_test], [y_train, y_test])\n",
    "    tfms  = [None, [Categorize()]]\n",
    "    dsets = TSDatasets(X, y, tfms=tfms, splits=splits, inplace=True)\n",
    "    dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=[TSStandardize()], num_workers=0)\n",
    "    model = InceptionTime(dls.vars, dls.c)\n",
    "    learn = Learner(dls, model, metrics=accuracy)\n",
    "    learn.fit_one_cycle(25, lr_max=1e-3)\n",
    "    learn.plot_metrics()\n",
    "    learn.save_all(path='export', dls_fname='dls_'+str(i)+'_'+str(j), model_fname='model_'+str(i)+'_'+str(j), learner_fname='learner_'+str(i)+'_'+str(j))\n",
    "#     display(type(X_train),X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key takeaway\n",
    "\n",
    "InceptionTime can do the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, splits = get_classification_data('LSST', split_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms  = [None, TSClassification()] # TSClassification == Categorize\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, new_y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=[64, 128])\n",
    "dls.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_list(data):\n",
    "    intervals=get_intervals(data)\n",
    "    std_list=[]\n",
    "    data.drop(columns=['index'])\n",
    "    for interval in intervals:\n",
    "        data_seg=data.iloc[interval[0]:interval[1],:].to_numpy()\n",
    "        std=data_seg.adj.std()\n",
    "        std_list.append(std)\n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = TSStandardize(by_sample=True)\n",
    "mv_clf = TSClassifier(X, y, splits=splits, path='models', arch=InceptionTimePlus, batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph())\n",
    "mv_clf.fit_one_cycle(10, 1e-2)\n",
    "mv_clf.export(\"mv_clf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
